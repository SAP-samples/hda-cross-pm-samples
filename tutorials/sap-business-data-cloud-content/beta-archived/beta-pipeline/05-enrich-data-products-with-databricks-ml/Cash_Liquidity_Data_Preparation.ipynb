{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f052a3d7-4d8b-45ab-910e-506c42894ae2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cash Liquidity Forecast\n",
    "For the Data Product Cash Flow we want to expand the data product by calculating for upcoming periods the cash flow. This notebook shows an example workflow for the enrichment of the CashFlow data product which is going to be exposed back to SAP Datasphere in Business Data Cloud (BDC).\n",
    "This involves in total the following steps for the overall prediction:\n",
    "- Consume exposed data product over the Delta Share\n",
    "- Prepare data for time series forecasting\n",
    "- Perform hyperparameter optimization for time series prediction with model selection\n",
    "- Log best model to MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae446cb3-b42e-408f-af2e-80b0acd70978",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Install packages\n",
    "All necessary packages for this notebook are going to be outlined in the following notebook cell. In order to make sure that the results are reproducible, the following packages are going to be installed.\n",
    "- Databricks feature engineering: Allows to store feature tables in Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9b5badb-a6fc-4423-aee5-aa5b0b62ddbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-feature-engineering\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b19b722-3c4d-418d-aa30-8a79a957ef1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02a63784-12dc-419a-b715-1760aaf66da5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient, FeatureLookup\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, col, date_trunc, sum, explode, sequence, min, max, lit, expr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51809028-88a2-4dd0-8dc9-44eedf5590bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Setup Spark Session and consume data product\n",
    "In order to isolate the created data assets, we create a catalog within Databricks and a respective schema within the catalog. Please replace the values <CATALOG_NAME> and <SCHEMA_NAME> with distinct values that provide you with a separated catalog and schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a794c4e-861f-4839-aa4f-c959ccf3752a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE CATALOG IF NOT EXISTS <CATALOG_NAME>;\n",
    "SET CATALOG <CATALOG_NAME>;\n",
    "CREATE SCHEMA IF NOT EXISTS <SCHEMA_NAME>;\n",
    "USE SCHEMA <SCHEMA_NAME>;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6befe50a-09a5-47ed-be95-593122c62755",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "From BDC we expose a [delta enabled local table](https://help.sap.com/docs/SAP_DATASPHERE/c8a54ee704e94e15926551293243fd1d/154bdffb35814d5481d1f6de143a6b9e.html?locale=en-US) over the delta share which provides us with a table containing multiple entries for the same primary key. The dataset contains the __OPERATION_TYPE column marking the transactional statement (Insert, Update, Delete) together with the __TIMESTAMP column marking when this change happened. As we want to use the Cashflow transactional statements, we transform our dataset in the following to provide the most recent entry per primary key. In case the most recent entry is a deletion, we filter this record out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f71be7eb-e564-4cfd-9028-34c7a45d9444",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"cash_flow_data_preparation\").getOrCreate()\n",
    "data = spark.read.table(<SHARE_TABLE_PATH>)\n",
    "data = data.alias(\"l\").\\\n",
    "    groupBy(col(\"CashFlowID\")).agg(max(col(\"__TIMESTAMP\")).alias(\"left__TIMESTAMP\")).\\\n",
    "    join(\n",
    "        data.alias(\"r\"),\n",
    "        (col(\"l.CashFlowID\") == col(\"r.CashFlowID\")) & (col(\"left__TIMESTAMP\") == col(\"r.__TIMESTAMP\")),\n",
    "        \"left\"\n",
    "    ).\\\n",
    "    where(\"'__OPERATION_TYPE' != 'D'\").\\\n",
    "    select(\"r.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a44a916-25ba-4eb8-bcf1-207c48bda79b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data preparation\n",
    "For the data preparation of the Cash Flow data product for the time series forecast, we remodel the data by performing the following steps. The date column is going to be the posting date as the posting marks whether a Cash flow is booked or not. The forecast is performed on a monthly date sequence on the posting date. [See details under term definition for posting date](https://help.sap.com/glossary/?locale=en-US&term=posting%2520date):\n",
    "1. Replace empty strings with Null values\n",
    "2. Select necessary columns and filter out on the Posting date invalid dates and Null values\n",
    "3. Floor Posting date column to month and rename date and value column\n",
    "4. Group data on date column and sum up Cash Flow per month\n",
    "5. Generate continuous time series range between minimum date and maximum date present in data\n",
    "6. Join generated time sequence to time series data in order to provide continuous time series dataframe\n",
    "7. Fill Null values with 0 as at those days no cash flow was recorded\n",
    "8. Convert Spark dataframe to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8d39828-61d1-4357-95f1-d52bf28707a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = data.replace('', None)\n",
    "data = data.select(\"CashFlowID\", \"PostingDate\", \"AmountInCompanyCodeCurrency\").where(\"PostingDate != '9999-12-31' or PostingDate IS NOT NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d393b966-ca5a-4705-940e-42677fb0ecec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Floor date and rename columns\n",
    "data = data.\\\n",
    "    withColumn(\"PostingDate\", date_trunc(\"month\", col(\"PostingDate\")).cast(\"date\")).\\\n",
    "    withColumnsRenamed({\"PostingDate\": \"ds\", \"Company_Code\": \"CompanyCode\", \"AmountInCompanyCodeCurrency\": \"y\"})\n",
    "# aggregate time series on date and sum cash flow\n",
    "time_series_data = data.\\\n",
    "    select(\"ds\", \"CompanyCode\", \"y\").\\\n",
    "    groupBy(\"ds\", \"CompanyCode\").\\\n",
    "    agg(sum(\"y\").alias(\"y\")).\\\n",
    "    orderBy(\"ds\")\n",
    "# generate continous time series sequence\n",
    "date_sequence_data = time_series_data\\\n",
    "    .select(\n",
    "        explode(\n",
    "            expr(\"sequence(min(ds), max(ds), INTERVAL 1 MONTH)\")\n",
    "            ).alias(\"ds\"))\n",
    "date_company_combination = time_series_data.select(\"CompanyCode\").\\\n",
    "    distinct().\\\n",
    "    join(date_sequence_data, how=\"cross\")\n",
    "# join time series data together with time series sequence\n",
    "time_series_data = time_series_data.\\\n",
    "    join(date_company_combination, on=[\"ds\", \"CompanyCode\"], how=\"right\").\\\n",
    "    fillna(0, subset=[\"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff161f0b-9c08-4a10-b2d1-88aa780e88b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Store prepared dataset to the Feature Store of Databricks\n",
    "In order to reuse the dataset for our Training as well our Prediction, we store the transformed dataset into the feature store Databricks. This provides the possibility to not repeat the same data preparation script for both the Training as well as the Prediction notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f471f65d-2a1e-4aa5-8496-ee0ed39d5859",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fe_client = FeatureEngineeringClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1e6127d-69c6-4f24-a39a-9d4595513494",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fe_client.create_table(\n",
    "    name=\"prepared_cash_flow_time_series\",\n",
    "    primary_keys=[\"ds\", \"CompanyCode\"],\n",
    "    schema=time_series_data.schema,\n",
    "    description=\"Prepared Cash Flow data product for time series forecasting\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dc03faf-d56b-4922-b2e4-ef5b114e5c26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fe_client.write_table(\n",
    "    name=\"prepared_cash_flow_time_series\",\n",
    "    df=time_series_data,\n",
    "    mode=\"merge\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 811068389050590,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Cash_Liquidity_Data_Preparation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
