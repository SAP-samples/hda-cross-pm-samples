{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c85f9a73-59c6-43b5-a976-649c84f43d30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cash Liquidity Forecast\n",
    "For the Data Product Cash Flow we want to expand the data product by calculating for upcoming periods the cash flow. This notebook shows an example workflow for the enrichment of the CashFlow data product which is going to be exposed back to SAP Datasphere in Business Data Cloud (BDC).\n",
    "This involves in total the following steps for the overall prediction:\n",
    "- Consume exposed data product over the Delta Share\n",
    "- Prepare data for time series forecasting\n",
    "- Perform hyperparameter optimization for time series prediction with model selection\n",
    "- Log best model to MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a9374b4-e697-4e5b-bacf-7de66f29d692",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Install packages\n",
    "All necessary packages for this notebook are going to be outlined in the following notebook cell. In order to make sure that the results are reproducible, the following packages are going to be installed:\n",
    "- mlflow: Tracking of our ML model\n",
    "- AutoTS: Time Series package that performs Hyper Parameter Tuning over multiple time series models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7db6fb6d-c2ca-4295-91b3-b9042a440ee6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow\n",
    "%pip install autots['additional']\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ca7399f-6baf-4a3c-ae62-5ea576082b78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6467884-2a25-49d8-bb20-4e35858fe424",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from autots import AutoTS\n",
    "from delta import *\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "from mlflow.client import MlflowClient\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import re\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3999981-2287-48a9-b59b-b1799c320b47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Setup Spark Session and consume prepared data product from feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbf60b2d-d75d-40e4-894b-35074dfcfa76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE CATALOG IF NOT EXISTS <CATALOG_NAME>;\n",
    "SET CATALOG <CATALOG_NAME>;\n",
    "CREATE SCHEMA IF NOT EXISTS <SCHEMA_NAME>;\n",
    "USE SCHEMA <SCHEMA_NAME>;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "482cbccf-aa1c-4f9e-854d-cfa0f2cf55af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature store table\n",
    "We read from our feature store the stored table `prepared_cash_flow_time_series` generated by our Data Preparation scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb30843c-14e0-4dad-ae15-f0c477063812",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "builder = SparkSession.builder.appName(\"Time Series Training\").getOrCreate()\n",
    "data = spark.read.table(\"prepared_cash_flow_time_series\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97e18790-667d-4fbd-9e49-1a1b72099a3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# convert dataframe to time series data with correct data types\n",
    "# ds: datetime datetype, y: float\n",
    "time_series_data = data.toPandas().astype({\"ds\": \"datetime64[ns]\", \"y\": float})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a04d4c5-04df-4d06-840d-af15bf55c6b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Time Series Forecasting\n",
    "We train our time series algorithm using the [AutoTS library](https://github.com/winedarksea/autots). It provides a possibility to run multiple models in parallel to run and validate different data preparation steps as well as different Time Series algorithms in parallel. For the overall aspects we log our model to MLflow under the experiment called `Time Series`. As the package AutoTS does not provide an autologging of the model, we create a prediction Python class, in order to be able to use the MLflow prediction capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a5b99e9-539c-4b9c-bbbf-e132a57ec345",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "root_path = str(Path(notebook_path).parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eeff26ea-c831-47cf-890d-60f5ccd7c5eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_experiment(f\"{root_path}/Time Series Forecasting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc4d41a7-e262-47fe-9e06-cd9b49690682",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class AutoTSPredictor(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"\n",
    "    A custom MLflow Python model wrapper for time series forecasting using the AutoTS model.\n",
    "\n",
    "    This class is designed to integrate an AutoTS forecasting model with MLflow's model\n",
    "    deployment capabilities. The `AutoTSPredictor` class inherits from `mlflow.pyfunc.PythonModel`,\n",
    "    allowing it to be used in MLflow pipelines or deployed as an MLflow model.\n",
    "\n",
    "    Args:\n",
    "        mlflow (mlflow.pyfunc.PythonModel): Base class for custom MLflow models.\n",
    "    \"\"\"\n",
    "    def __init__(self, autots_model: AutoTS):\n",
    "        \"\"\"\n",
    "        Initializes the AutoTSPredictor with a trained AutoTS model.\n",
    "\n",
    "        Args:\n",
    "            autots_model (AutoTS): An instance of the AutoTS model that has been trained\n",
    "                for time series forecasting.\n",
    "        \"\"\"\n",
    "        self.autots_model = autots_model\n",
    "    def predict(self, context, model_input) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generates forecasts using the AutoTS model and formats the output.\n",
    "\n",
    "        The `predict` method leverages the `AutoTS` model to produce forecasts,\n",
    "        including the central forecast and optional upper and lower bounds for\n",
    "        prediction intervals.\n",
    "\n",
    "        Args:\n",
    "            context: MLflow context, provided during deployment; typically unused here.\n",
    "            model_input: Model input provided to the MLflow model (not used in this function).\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing the forecasted values along with\n",
    "            upper (`y_high`) and lower (`y_low`) forecast bounds.\n",
    "\n",
    "        The returned DataFrame has the following columns:\n",
    "            - `y`: The central forecast values.\n",
    "            - `y_high`: The upper bound of the forecast interval.\n",
    "            - `y_low`: The lower bound of the forecast interval.\n",
    "        \"\"\"\n",
    "        predictions = self.autots_model.predict()\n",
    "        forecasts_df = predictions.forecast.reset_index(names=[\"date\"])\n",
    "        forecasts_df = forecasts_df.melt(id_vars=[\"date\"], var_name=\"CompanyCode\", value_name=\"forecast\")\n",
    "        upper_forecast = predictions.upper_forecast.reset_index(names=[\"date\"])\n",
    "        upper_forecast = upper_forecast.melt(id_vars=[\"date\"], var_name=\"CompanyCode\", value_name=\"upper_forecast\")\n",
    "        lower_forecast = predictions.lower_forecast.reset_index(names=[\"date\"])\n",
    "        lower_forecast = lower_forecast.melt(id_vars=[\"date\"], var_name=\"CompanyCode\", value_name=\"lower_forecast\")\n",
    "        prediction_dataframes = [forecasts_df, upper_forecast, lower_forecast]\n",
    "        prediction_merged = reduce(lambda left, right: pd.merge(left, right, on=[\"date\", \"CompanyCode\"], how=\"inner\"), prediction_dataframes)\n",
    "        return prediction_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6931a78-4de8-44e6-8256-49ca5337396f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For the training with the AutoTS package, we use in total 4 different parameters which can be changed based on the individual characteristics of the data:\n",
    "- FORECAST_LENGTH: Number of periods over which to evaluate forecast. Can be overriden later in `.predict`. when you don’t have much historical data, using a small forecast length for `.fit` and the full desired forecast length for `.predict` is usually the best possible approach given limitations.\n",
    "- MAX_GENERATIONS: Number of genetic algorithms generations to run. More runs = longer runtime, generally better accuracy. \n",
    "- NUM_VALIDATIONS: Number of cross validations to perform. 0 for just train/test on best split. Possible confusion: `num_validations` is the number of validations to perform after the first eval segment, so totally eval/validations will be this + 1. Also **auto** and **max** aliases available. Max maxes out at **50**.\n",
    "- ENSEMBLE: None or list or comma-separated string containing: **auto**, **simple**, **distance**, **horizontal**, **horizontal-min**, **horizontal-max**, **mosaic**, **subsample**\n",
    "- NUM_JOBS: number of jobs used for performing the hyperparameter tuning for AutoTS\n",
    "- MODEL_LIST: Model name list with the associated models. Under the method `from autots.models.model_list import model_lists` a dictonary is returned showing all the different available models under the specific dictionary key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec428cbc-cc69-4994-863f-c619639c5b02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from autots.models.model_list import model_lists\n",
    "model_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9840cebb-aede-4d0f-8cb3-0375a351640a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "FORECAST_LENGTH = 6\n",
    "MAX_GENERATIONS = 2\n",
    "NUM_VALIDATIONS = 2\n",
    "ENSEMBLE = \"simple\"\n",
    "NUM_JOBS = 30\n",
    "MODEL_LIST = \"fast\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b7bf7e2-3950-4180-9049-ace23024d33c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run() as run:\n",
    "    try:\n",
    "        model = AutoTS(forecast_length=FORECAST_LENGTH,\n",
    "                    max_generations=MAX_GENERATIONS,\n",
    "                    num_validations=NUM_VALIDATIONS,\n",
    "                    ensemble=ENSEMBLE,\n",
    "                    verbose=0,\n",
    "                    n_jobs=NUM_JOBS,\n",
    "                    model_list=MODEL_LIST)\n",
    "        model.fit(time_series_data, date_col=\"ds\", value_col=\"y\", id_col=\"CompanyCode\")\n",
    "        # Save the AutoTS model as a serialized file\n",
    "        model_path = \"autots_model.pkl\"\n",
    "        with open(model_path, \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "\n",
    "        # Log the AutoTS model as an artifact\n",
    "        mlflow.log_artifact(model_path)\n",
    "        # Log the custom model with MLflow's pyfunc API\n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=\"model\",\n",
    "            python_model=AutoTSPredictor(model),\n",
    "            artifacts={\"autots_model\": model_path},\n",
    "            input_example=time_series_data[:5],\n",
    "            registered_model_name=\"cashflow_ts_model\"\n",
    "        )\n",
    "        model_uri = f\"runs:/{run.info.run_id}/model\"\n",
    "        best_model_id = model.best_model_id\n",
    "        print(f\"Model saved in MLflow Run ID: {run.info.run_id}\")\n",
    "        os.remove(model_path)\n",
    "    except:\n",
    "        raise ValueError(\"The provided data does not provide a coherent time series that allows the models to generalize. Please provide a time series with at least 3 years of data and check, whether the prepared dataset contains a lot of 0 values due to gaps per month or seasonality issues.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1875395145511562,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Cash_Liquidity_Training",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
