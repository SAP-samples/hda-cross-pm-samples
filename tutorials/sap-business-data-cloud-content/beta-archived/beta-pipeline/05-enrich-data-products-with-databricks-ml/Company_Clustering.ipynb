{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95ce4d4b-342d-45c5-8aad-890c56b07969",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Company data clustering\n",
    "Based on the company data product we consume it in order to cluster the data. To provide the data to the cluster algorithm, we prepare the data by handling Null values and afterwards perform a One Hot Encoding in order to get a dataframe only containing numerical values. Prior to applying the model, we optimize selected hyperparameters of the Affinity Propagation clustering algorithm. After performing the hyperparameter tuning, we select the model with the highest silhouette score. This model is used in order to then apply the model for the clustering approach. In order to be able to visualize the data in a dashboard, we perform a PCA to reduce the dimensionality of the prepared dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2926420-d323-4186-8623-3c10aed5ba59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Install packages\n",
    "All necessary packages for this notebook are going to be outlined in the following notebook cell. In order to make sure that the results are reproducible, the following package versions are going to be installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90fe0d16-36c8-4bf3-81fd-59318a6a0422",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow\n",
    "%pip install bayesian-optimization\n",
    "%pip install databricks-feature-engineering\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f036530b-eef4-469e-a114-390f2d12855a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea668f41-c6dc-4804-bf5d-b9b5ba7f4bf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, max\n",
    "from pyspark.sql.types import DecimalType\n",
    "import pandas as pd\n",
    "from delta import *\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "from pathlib import Path\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "from sklearn.metrics.cluster import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b93a092c-bd90-4695-843c-8178ad3310f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Setup Spark Session and consume data product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc57a1e1-c418-40b2-9511-d5775d419369",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE CATALOG IF NOT EXISTS <CATALOG_NAME>;\n",
    "SET CATALOG <CATALOG_NAME>;\n",
    "CREATE SCHEMA IF NOT EXISTS <SCHEMA_NAME>;\n",
    "USE SCHEMA <SCHEMA_NAME>;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f820fd72-0992-4e63-a25c-c3d04125d43d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Please replace the value `<share_table_path>` with the path of the table to query from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6468b319-4e8d-413f-8cf2-47c91b53f5bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"company_clustering\").getOrCreate()\n",
    "data = spark.read.table(<share_table_path>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd3178da-d94d-42f9-aa45-75d4d4de2a06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data preparation\n",
    "When consuming the dataset, we select a subset of the company data product which does not consider columns that contain unique identifiers. In order to prepare the company dataset, we need to transform the boolean and character columns into a numeric representation. For the boolean columns we replace the `True` value to 1 and the `False` value to 0. For the character columns we encode the values by applying a One Hot Encoder. In case a column only contains null values, we remove the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2ff3ebf-71a8-40df-8102-acb5344a3a03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def drop_fully_null_columns(df, but_keep_these=[]):\n",
    "    \"\"\"Drops DataFrame columns that are fully null\n",
    "    (i.e. the maximum value is null)\n",
    "\n",
    "    Arguments:\n",
    "        df {spark DataFrame} -- spark dataframe\n",
    "        but_keep_these {list} -- list of columns to keep without checking for nulls\n",
    "\n",
    "    Returns:\n",
    "        spark DataFrame -- dataframe with fully null columns removed\n",
    "    \"\"\"\n",
    "\n",
    "    # skip checking some columns\n",
    "    cols_to_check = [col for col in df.columns if col not in but_keep_these]\n",
    "    if len(cols_to_check) > 0:\n",
    "        # drop columns for which the max is None\n",
    "        rows_with_data = df.select(*cols_to_check).groupby().agg(*[max(c).alias(c) for c in cols_to_check]).take(1)[0]\n",
    "        cols_to_drop = [c for c, const in rows_with_data.asDict().items() if const == None]\n",
    "        cleaned_df = df.drop(*cols_to_drop)\n",
    "\n",
    "        return cleaned_df\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6210c83-8506-4653-b8fd-853bc6e3dbae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "clustering_data = data.select(\"CompanyCode\", \"Country\", \"Currency\", \"Language\", \"ControllingArea\", \"CreditControlArea\", \"FiscalYearVariant\", \"FieldStatusVariant\", \"TaxRptgDateIsActive\", \n",
    "                              \"DocDateIsUsedForTaxDetn\", \"FinancialManagementArea\", \"ExtendedWhldgTaxIsActive\", \"CashDiscountBaseAmtIsNetAmt\", \"NonTaxableTransactionTaxCode\")\n",
    "clustering_data = clustering_data.replace('', None)\n",
    "converted_data_types = {column: col(column).cast('integer').alias(column) for column, column_dtype in clustering_data.dtypes if column_dtype == 'boolean'}\n",
    "clustering_data = clustering_data.withColumns(converted_data_types)\n",
    "clustering_data = drop_fully_null_columns(clustering_data)\n",
    "replace_string_values = {column: f\"No{column}\" for column, col_dtypes in clustering_data.dtypes if col_dtypes == \"string\" and column != \"CompanyCode\"}\n",
    "clustering_data = clustering_data.fillna(replace_string_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a032e0e-da5a-46fd-9a7e-341a72fe1f7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In order to apply the dataset, we transform the prepared Spark dataframe to a Pandas dataframe. The column Company Code, which is the primary key is going to be removed and assigned to a separate variable. we perform the One Hot Encoding over the pandas library functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28060894-139a-4f99-9c1d-10d30b823a1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Hyperparameter tuning\n",
    "In order to select an optimal set of hyperparameters, we apply a Bayesian Search optimization in order to determine an optimal set of parameter combination. We want to therefore maximize the silhouette score. As a clustering algorithm we use the Affinity Propagation from scikit-learn. For the Bayesian Search optimization we define the following parameters:\n",
    "- random_iteration (number of iteration for random selection of hyperparameters to define search space)\n",
    "- optimization_iteration (number of optimization iteration of hyperparamter tuning process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85994ba5-3248-4842-8068-1ae5e15bedc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "clustering_data = clustering_data.toPandas()\n",
    "company_code_data = clustering_data.pop(\"CompanyCode\")\n",
    "clustering_data = pd.get_dummies(data=clustering_data, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad6e8b0c-845e-4ad5-8df2-038eaad1311f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def cluster_scoring(cluster_labels:np.array, data: pd.DataFrame) -> tuple:\n",
    "    \"\"\"\n",
    "    Computes cluster evaluation metrics for a given set of cluster labels and data points.\n",
    "\n",
    "    This method calculates three clustering quality metrics:\n",
    "    - Silhouette Score: Measures how similar an object is to its own cluster compared to other clusters.\n",
    "    - Calinski-Harabasz Index: Evaluates the ratio of between-cluster dispersion to within-cluster dispersion.\n",
    "    - Davies-Bouldin Index: Measures the average similarity ratio of each cluster with the one most similar to it.\n",
    "\n",
    "    Args:\n",
    "        cluster_labels (np.array): An array of cluster labels for each data point, as assigned by a clustering algorithm.\n",
    "        data (pd.DataFrame): The dataset used for clustering, where each row represents a data point and each column represents a feature.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - silhouette_scoring (float): The Silhouette Score (higher is better, -1 if there is only one cluster).\n",
    "            - calinski_score (float): The Calinski-Harabasz Index (higher is better, 0 if there is only one cluster).\n",
    "            - davies_score (float): The Davies-Bouldin Index (lower is better, infinity if there is only one cluster).\n",
    "    \"\"\"\n",
    "    number_labels = len(set(cluster_labels))\n",
    "    if number_labels > 1:\n",
    "        silhouette_scoring = silhouette_score(data, labels=cluster_labels)\n",
    "        calinski_score = calinski_harabasz_score(data, cluster_labels)\n",
    "        davies_score = davies_bouldin_score(data, cluster_labels)\n",
    "    else:\n",
    "        silhouette_scoring, calinski_score, davies_score = -1, 0, np.inf\n",
    "    return silhouette_scoring, calinski_score, davies_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b796785a-02da-4438-ab31-96cec6c43c40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def affinity_clustering_hpt(damping: float, max_iter: float, convergence_iter: float, data: pd.DataFrame=clustering_data):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning and model training using the Affinity Propagation clustering algorithm.\n",
    "\n",
    "    This method initializes an Affinity Propagation model with the specified parameters, fits it to the provided dataset,\n",
    "    and evaluates clustering performance using silhouette, Calinski-Harabasz, and Davies-Bouldin scores. Model and metrics\n",
    "    are logged to MLflow for tracking purposes.\n",
    "\n",
    "    Args:\n",
    "        damping (float): The damping factor between 0.5 and 1.0 that adjusts the extent to which the responsibility and availability\n",
    "                         messages are updated with each iteration in the Affinity Propagation algorithm.\n",
    "        max_iter (float): The maximum number of iterations for the model to converge. This value is rounded to the nearest integer.\n",
    "        convergence_iter (float): The number of iterations with no change in the estimated clusters before convergence is\n",
    "                                  declared. This value is rounded to the nearest integer.\n",
    "        data (pd.DataFrame, optional): The dataset to fit the model on. Defaults to `clustering_data`.\n",
    "\n",
    "    Returns:\n",
    "        float: The silhouette score evaluating clustering quality, where a higher score indicates better-defined clusters.\n",
    "    \"\"\"\n",
    "    max_iter = int(round(max_iter, 0))\n",
    "    convergence_iter = int(round(convergence_iter, 0))\n",
    "    with mlflow.start_run():\n",
    "        affinity_model = AffinityPropagation(damping=damping, max_iter=max_iter, convergence_iter=convergence_iter)\n",
    "        cluster_labels = affinity_model.fit_predict(data)\n",
    "        silhouette_scoring, calinski_score, davies_score = cluster_scoring(cluster_labels, data)\n",
    "        input_example = data[:5]\n",
    "        signature = infer_signature(input_example, model_output=cluster_labels)\n",
    "        mlflow.sklearn.log_model(affinity_model, artifact_path=\"model\", signature=signature)\n",
    "        mlflow.log_params(affinity_model.get_params())\n",
    "        mlflow.log_param(\"clustering_model\", \"Affinity Propagation\")\n",
    "        mlflow.log_metrics({\"silhouette_score\": silhouette_scoring, \"calinski_harabasz_score\": calinski_score, \"davies_bouldin_score\": davies_score})\n",
    "    return silhouette_scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41d350a2-b70d-4ed4-8d87-3a6996e7bb4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "random_iteration = 5\n",
    "optimization_iteration = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5de19821-629e-4022-acf1-d200d4b4bceb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Define MLflow setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfbd08c3-ab3d-427a-899e-7b345c83c2e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "root_path = str(Path(notebook_path).parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abd11f62-a371-43d3-a2ba-ccd5e7033b68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_experiment(f\"{root_path}/Company clustering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "474a71b3-3ed2-4a0f-9f11-e2a9e03a7d47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Run Hyperparameter optimization\n",
    "For the Affinity Propagation we define the following Hyperparameters and the respective search range:\n",
    "- damping: [0.5, 0.99999]\n",
    "- max_iter: [10, 4000]\n",
    "- convergence_iter: [10, 4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5a0d795-ec83-4760-a377-da2a419d5e85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "affinity_propagation_parameter_bound = {\"damping\": (0.5, 0.99999), \"max_iter\": (10, 4000), \"convergence_iter\": (10, 4000)}\n",
    "ap_bo_optimizer = BayesianOptimization(\n",
    "    f=affinity_clustering_hpt,\n",
    "    pbounds=affinity_propagation_parameter_bound,\n",
    "    random_state=42\n",
    ")\n",
    "ap_bo_optimizer.maximize(init_points=random_iteration, n_iter=optimization_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e006b08b-76bc-42c8-99cc-ccf5fc71e3ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Retrieval of runs after hyperparameter tuning\n",
    "In order to retrieve the runs from the hyperparameter tuning, we search runs under the used experiment. We will retrieve the run with the highest silhouette score to perform our predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5606dd4-1e4d-48df-b5ea-1b76bbb04905",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cluster_runs = mlflow.search_runs()\n",
    "run_id = cluster_runs.sort_values(\"metrics.silhouette_score\", ascending=False).head(1).loc[:,\"run_id\"].tolist()[0]\n",
    "logged_model = f\"runs:/{run_id}/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d03fee2e-1120-462f-9e78-4c2f9d436cb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "clustering_prediction_data = np.ascontiguousarray(clustering_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fda173aa-b42f-4e8d-9b8a-e181a93d62e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load model as a Spark UDF. Override result_type if the model does not return double values.\n",
    "loaded_model = mlflow.sklearn.load_model(model_uri=logged_model)\n",
    "cluster_labels = loaded_model.predict(clustering_prediction_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2841b8b2-0990-432e-abb8-3f6192edcadb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tsne_model = TSNE(n_components=2)\n",
    "tsne_data = tsne_model.fit_transform(clustering_data)\n",
    "tsne_data = pd.DataFrame(tsne_data, columns=[\"TSNE_X\", \"TSNE_Y\"])\n",
    "tsne_data[\"labels\"] = cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b3f6802-60b8-424f-a30d-5f1bc8736fd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "company_clusters = pd.concat([tsne_data, company_code_data], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6d3e4ba-b1a8-4fa9-8d8e-718c0a55c537",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS company_code_clusters (\n",
    "  TSNE_X DECIMAL(38,18),\n",
    "  TSNE_Y DECIMAL(38,18),\n",
    "  labels LONG,\n",
    "  CompanyCode STRING,\n",
    "  PRIMARY KEY (CompanyCode)\n",
    ");\n",
    "ALTER TABLE company_code_clusters SET TBLPROPERTIES (\n",
    "  delta.enableChangeDataFeed = true,\n",
    "  delta.enableDeletionVectors = false\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_clusters = spark.createDataFrame(company_clusters)\n",
    "company_clusters = company_clusters.withColumns({\"TSNE_X\": col(\"TSNE_X\").cast(DecimalType(38,18)), \"TSNE_Y\": col(\"TSNE_Y\").cast(DecimalType(38,18))})\n",
    "company_clusters.write.format(\"delta\").\\\n",
    "    mode(\"overwrite\").\\\n",
    "    option(\"delta.enableChangeDataFeed\", \"true\").\\\n",
    "    option(\"delta.enableDeletionVectors\", \"false\").\\\n",
    "    saveAsTable(\"Company_Code_Clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d994896-c071-4c3e-99ae-74d1c1f2bc4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SHARE IF NOT EXISTS <SHARE_NAME>;\n",
    "ALTER SHARE <SHARE_NAME> ADD TABLE Company_Code_Clusters WITH HISTORY;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill out the following parameters below in order to generate the CSN and ORD documents:\n",
    "- Share Name: Provide the exact name of the Delta Share name\n",
    "- ORD Title: Provide a concise name that is the name of the data product (**This is the name which is visible in the Datasphere catalog**)\n",
    "- Short_Description: Provide a short description for the ORD document\n",
    "- Description: Provide a description for the ORD document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34ff8a72-b75b-4d81-9112-755b845bee11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from generate_ord_and_csn import ord_csn_generation\n",
    "share_name = <SHARE_NAME>\n",
    "ord_title = <ORD_TITLE>\n",
    "short_description = <SHORT_DESCRIPTION>\n",
    "description = <DESCRIPTION>\n",
    "\n",
    "ord_csn_generation(share_name = share_name, ord_title=ord_title, ord_short_description = short_description, ord_description = description)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1875395145511645,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Company_Clustering",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
